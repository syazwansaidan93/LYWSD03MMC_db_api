import asyncio
import sys
import json
import sqlite3
from datetime import datetime, timedelta
from bleak import BleakClient, BleakScanner
from bleak.exc import BleakError
import os # For robust path handling
import logging # Added for improved logging

# --- Configure Logging ---
logging.basicConfig(
    level=logging.INFO, # Set to logging.DEBUG for more verbose output
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("sensor_collector.log"), # Log to a file
        logging.StreamHandler(sys.stdout) # Also log to console
    ]
)

CF = "config.json"
DB_NAME = "sensor_data.db"
RETENTION_DAYS = 7  # 1 week retention

# BLE specific constants
RT = 20   # Connection Timeout
MDR = 3   # Max Device Retries for discovery (not directly used in current find_device_by_address)
DRD = 5   # Device Retry Delay for discovery (not directly used in current find_device_by_address)
D_C_U = "ebe0ccc1-7a0a-4b0c-8a1a-6ff2997da3a6" # Data Characteristic UUID (Temp & Humidity)

# --- Global Data Stores ---
# last_saved_time tracks when the last data point was successfully written to the DB
# for the single sensor. It's primarily for informational logging.
last_saved_time = datetime.min

# --- Configuration Loading ---
try:
    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(script_dir, CF)
    with open(config_path, 'r') as f:
        c_d = json.load(f)
except FileNotFoundError:
    logging.error(f"Error: {CF} not found. Please create it with 'mac_addresses'.")
    sys.exit(1)
except json.JSONDecodeError:
    logging.error(f"Error: Could not parse {CF}. Check JSON format.")
    sys.exit(1)

MACS = c_d.get("mac_addresses", [])
if not MACS:
    logging.error("Error: No MAC addresses found in config.json.")
    sys.exit(1)
if len(MACS) > 1:
    logging.warning("Warning: config.json contains more than one MAC address. This script is optimized for a single sensor and will only process the first one found.")
    MACS = [MACS[0]]

# Normalize MAC address to uppercase for consistency
mac_to_monitor = MACS[0].upper() # Applied Device Address Normalization

# Load DATA_SAVE_INTERVAL_MINUTES from config.json, with a default of 15
DATA_SAVE_INTERVAL_MINUTES = c_d.get("poll_interval_minutes", 15) # Applied Configuration Consistency
COLLECTION_INTERVAL_SECONDS = DATA_SAVE_INTERVAL_MINUTES * 60

# --- Database Functions ---
def get_db_connection():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    db_path = os.path.join(script_dir, DB_NAME)
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    return conn

def setup_database():
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS sensor_readings (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp DATETIME NOT NULL,
            temperature REAL,
            humidity INTEGER
        )
    ''')
    cursor.execute('''
        -- Index only on timestamp now, since mac_address is gone
        CREATE INDEX IF NOT EXISTS idx_timestamp ON sensor_readings (timestamp)
    ''')
    conn.commit()
    conn.close()
    logging.info("Database setup complete (no mac_address column).")

def store_sensor_data(temperature, humidity):
    """Stores a single temperature and humidity reading."""
    global last_saved_time # Declare global to modify the variable
    conn = get_db_connection()
    cursor = conn.cursor()
    current_time = datetime.now()

    try:
        cursor.execute('''
            INSERT INTO sensor_readings (timestamp, temperature, humidity)
            VALUES (?, ?, ?)
        ''', (current_time.strftime('%Y-%m-%d %H:%M:%S.%f'), temperature, humidity))
        conn.commit()
        last_saved_time = current_time # Update global last_saved_time
        logging.info(f"Saved data: T={temperature:.2f} H={humidity}.")
    except Exception as e:
        logging.error(f"Error storing data: {e}")
    finally:
        conn.close()

def apply_retention_policy():
    conn = get_db_connection()
    cursor = conn.cursor()
    threshold_time = datetime.now() - timedelta(days=RETENTION_DAYS)
    try:
        cursor.execute('DELETE FROM sensor_readings WHERE timestamp < ?', (threshold_time.strftime('%Y-%m-%d %H:%M:%S.%f'),))
        conn.commit()
        logging.info(f"Applied retention policy: Deleted data older than {RETENTION_DAYS} days.")
    except Exception as e:
        logging.error(f"Error applying retention policy: {e}")
    finally:
        conn.close()

# --- BLE Collector Functions ---

async def collect_single_reading(mac_address):
    """
    Connects to the sensor, enables notifications, waits for one data push,
    then disables notifications and disconnects.
    Returns (temperature, humidity) or (None, None) on failure.
    """
    client = None
    collected_data = None
    data_event = asyncio.Event() # Event to signal when data is received

    def notification_handler(sender, data):
        nonlocal collected_data # Use nonlocal to modify collected_data in outer scope
        if len(data) >= 3:
            temp = int.from_bytes(data[0:2], 'little', signed=True) / 100.0
            humid = data[2]
            collected_data = (temp, humid)
            data_event.set() # Signal that data has been received

    try:
        logging.info(f"Scanning for {mac_address}...")
        device = await BleakScanner.find_device_by_address(mac_address, timeout=10.0)
        if not device:
            logging.warning(f"Device {mac_address} not found.")
            return None, None

        logging.info(f"Connecting to {mac_address}...")
        client = BleakClient(device, timeout=RT)
        await client.connect()

        if not client.is_connected:
            logging.error(f"Failed to connect to {mac_address}.")
            return None, None

        logging.info(f"Connected to {mac_address}. Starting notifications...")
        await client.start_notify(D_C_U, notification_handler)

        # Wait for a short period for the sensor to push data
        try:
            await asyncio.wait_for(data_event.wait(), timeout=10.0) # Wait up to 10 seconds for data
        except asyncio.TimeoutError:
            logging.warning(f"Timeout waiting for data from {mac_address}.")
            collected_data = None # Ensure it's None if timeout occurs

        logging.info(f"Data collected (or timeout) from {mac_address}. Stopping notifications...")
        await client.stop_notify(D_C_U)

        if collected_data:
            logging.info(f"Received data: T={collected_data[0]:.2f} H={collected_data[1]} from {mac_address}")
            return collected_data
        else:
            return None, None

    except asyncio.TimeoutError:
        logging.error(f"Connection or operation timeout for {mac_address}.")
        return None, None
    except BleakError as e:
        logging.error(f"Bleak error during collection for {mac_address}: {e}")
        return None, None
    except Exception as e:
        logging.error(f"Unexpected error during collection for {mac_address}: {e}")
        return None, None
    finally:
        if client and client.is_connected:
            try:
                await client.disconnect()
                logging.info(f"Disconnected from {mac_address}.")
            except Exception as e:
                logging.error(f"Error during disconnect for {mac_address}: {e}")

async def collector_loop():
    """Main loop for periodic sensor data collection."""
    while True:
        temp, humid = await collect_single_reading(mac_to_monitor)

        if temp is not None and humid is not None:
            store_sensor_data(temp, humid)
        else:
            logging.warning(f"Failed to get data from {mac_to_monitor}. Will retry next interval.")

        logging.info(f"Waiting for {DATA_SAVE_INTERVAL_MINUTES} minutes until next collection...")
        await asyncio.sleep(COLLECTION_INTERVAL_SECONDS) # Wait for the next interval

async def retention_loop():
    """Periodically applies the data retention policy."""
    while True:
        await asyncio.sleep(24 * 3600) # Run once every 24 hours (86400 seconds)
        apply_retention_policy()

async def main():
    """Main entry point for the collector."""
    setup_database()
    asyncio.create_task(retention_loop())
    await collector_loop() # This loop runs indefinitely

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("\nScript interrupted by user. Exiting.")
